{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcc760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac182304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from common.utils.pickling import pickle_read, pickle_write\n",
    "from common.utils.misc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from prepare_submission_data import pkl_to_tar\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a5b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan:\n",
    "#+ load val embeddings\n",
    "#+ cluster val embeddings\n",
    "#+ assign avg loss to each cluster\n",
    "#+ load train embeddings\n",
    "#+ assign train embeddings to clusters\n",
    "#+ check if every cluster has representation\n",
    "# divide train embeddings by percentiles of distance from cluster centers\n",
    "# sample from each cluster - decide fro which percentile and assign weight to each cluster to affect how many samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231344a3",
   "metadata": {},
   "source": [
    "#### load val embeddings and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ccb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "p_val_loss = '/mnt/ext/shared/Projects/GNNetworkingChallenge/trained_oracle_models/6.33/losses_09-6.33/val_sample_loss_09-6.33.csv'\n",
    "val = load_sample_loss_csv(p_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4391787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "p_val_emb = '/mnt/ext/shared/Projects/GNNetworkingChallenge/trained_oracle_models/6.33/sample_embeddings_09-6.33/val_min_max_mean.pkl'\n",
    "emb_val = pickle_read(p_val_emb)\n",
    "assert(val.path == emb_val['paths']).all()\n",
    "emb_val = emb_val['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabbb528",
   "metadata": {},
   "source": [
    "#### load train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a8ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['1', '10', '11', '15', '5', '8', 'hard1', 'hard2', 'hard3',\n",
       "        'hard4', 'hard5'], dtype=object),\n",
       " (134347, 288))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tr_emb = '/mnt/ext/shared/Projects/GNNetworkingChallenge/trained_oracle_models/6.33/sample_embeddings_09-6.33/train_min_max_mean.pkl'\n",
    "emb_tr = pickle_read(p_tr_emb)\n",
    "p2e_tr = {p: i for i, p in enumerate(emb_tr['paths'])}\n",
    "\n",
    "tr = pd.DataFrame({'path': emb_tr['paths']})\n",
    "tr['dset'] = tr.path.str.split('/').str[7]\n",
    "\n",
    "# filter desired samples\n",
    "tr = tr[tr.dset.isin(['1', '10', '11', '15', '5', '8', 'hard1', 'hard2', 'hard3', 'hard4', 'hard5'])]\n",
    "emb_tr = emb_tr['embeddings'][tr.index.values]\n",
    "tr.dset.unique(), emb_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79510e6",
   "metadata": {},
   "source": [
    "#### cluster val embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13de60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 15\n",
    "kmeans = KMeans(n_clusters=nclusters)\n",
    "clusters = kmeans.fit_predict(emb_val)\n",
    "# assign to each val sample its cluster\n",
    "val['c'] = clusters\n",
    "# assign avg loss to each cluster\n",
    "clusters = val.groupby('c').loss.mean().to_frame()\n",
    "clusters['nval'] = val.groupby('c').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce299e8",
   "metadata": {},
   "source": [
    "#### assign train embeddings to val clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537188e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sample distances from each cluster\n",
    "trcd = kmeans.transform(emb_tr)\n",
    "\n",
    "# cluster assignment\n",
    "tr['c'] = trcd.argmin(axis=1)\n",
    "# distance from cluster center\n",
    "tr['cdist'] = trcd[np.arange(len(trcd)), tr['c'].values]\n",
    "clusters['ntrain'] = tr.groupby('c').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572e40cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>c</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>6.368943</td>\n",
       "      <td>2.650684</td>\n",
       "      <td>1.078728</td>\n",
       "      <td>7.10079</td>\n",
       "      <td>11.359393</td>\n",
       "      <td>13.822597</td>\n",
       "      <td>5.02397</td>\n",
       "      <td>4.664109</td>\n",
       "      <td>1.604214</td>\n",
       "      <td>18.740255</td>\n",
       "      <td>1.049908</td>\n",
       "      <td>5.37105</td>\n",
       "      <td>2.210074</td>\n",
       "      <td>16.746688</td>\n",
       "      <td>2.351897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nval</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntrain</th>\n",
       "      <td>15506.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>8233.000000</td>\n",
       "      <td>2185.00000</td>\n",
       "      <td>11586.000000</td>\n",
       "      <td>5302.000000</td>\n",
       "      <td>29670.00000</td>\n",
       "      <td>14409.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>1265.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3101.00000</td>\n",
       "      <td>19051.000000</td>\n",
       "      <td>9520.000000</td>\n",
       "      <td>14076.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "c                 0           1            2           3             4   \\\n",
       "loss        6.368943    2.650684     1.078728     7.10079     11.359393   \n",
       "nval       10.000000   21.000000     7.000000    13.00000     12.000000   \n",
       "ntrain  15506.000000  213.000000  8233.000000  2185.00000  11586.000000   \n",
       "\n",
       "c                5            6             7           8            9   \\\n",
       "loss      13.822597      5.02397      4.664109    1.604214    18.740255   \n",
       "nval       5.000000      7.00000      2.000000    2.000000    11.000000   \n",
       "ntrain  5302.000000  29670.00000  14409.000000  226.000000  1265.000000   \n",
       "\n",
       "c              10          11            12           13            14  \n",
       "loss     1.049908     5.37105      2.210074    16.746688      2.351897  \n",
       "nval    12.000000     6.00000      7.000000     5.000000     10.000000  \n",
       "ntrain   4.000000  3101.00000  19051.000000  9520.000000  14076.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaba9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert((clusters.ntrain > 0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64019537",
   "metadata": {},
   "source": [
    "#### select top closest from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516b6dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c\n",
       "0     20\n",
       "1     20\n",
       "2     20\n",
       "3     20\n",
       "4     20\n",
       "5     20\n",
       "6     20\n",
       "7     20\n",
       "8     20\n",
       "9     20\n",
       "10     4\n",
       "11    20\n",
       "12    20\n",
       "13    20\n",
       "14    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort before group so each group is sorted by distance from cluster center\n",
    "gp = tr.sort_values(by='cdist', ascending=True).groupby('c')\n",
    "# take top samples from each group\n",
    "ntop = 20\n",
    "top_samples = gp.head(ntop)\n",
    "gp = top_samples.groupby('c')\n",
    "# size of each resulting group\n",
    "gp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8804d",
   "metadata": {},
   "source": [
    "#### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3e2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_by_group(df, field):\n",
    "    \"\"\" groupby field and shuffle groups \"\"\"\n",
    "    groups = [df for _, df in df.groupby(field)]\n",
    "    random.shuffle(groups)\n",
    "    return pd.concat(groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bef3ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/ext-10g/users/yakovl/dev/GNNetworkingChallenge/subset_training/unshuffled_order/kmeans/ncls_15_top_20_data_1_10_11_15_5_8_h1_h2_h3_h4_h5')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataname = '_'.join(c.replace('hard','h') for c in sorted(tr.dset.unique()))\n",
    "name = f'ncls_{nclusters}_top_{ntop}_data_{dataname}'\n",
    "save_root = Path('/mnt/ext-10g/users/yakovl/dev/GNNetworkingChallenge/subset_training/unshuffled_order') / 'kmeans' / name\n",
    "if not save_root.exists():\n",
    "    save_root.mkdir()\n",
    "save_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "932ca3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'nclusters': nclusters,\n",
    "    'ntop': ntop,\n",
    "    'data': tr.dset.unique().tolist(),\n",
    "    'data_samples': len(tr),\n",
    "    'clusters': clusters,\n",
    "    'centers': kmeans.cluster_centers_,\n",
    "    'tr_embeddings': p_tr_emb,\n",
    "    'val_embeddings': p_val_emb,\n",
    "    'val_loss': p_val_loss,\n",
    "    'top_samples': top_samples,\n",
    "}\n",
    "pickle_write(save_root / 'config.pkl', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb4598e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments:\n",
    "# equal from each group, \n",
    "# by loss, loss**.5 loss**2\n",
    "# x shuffle samples | shuffle groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bfd944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = top_samples.groupby('c')\n",
    "ntries = 10\n",
    "\n",
    "iexp = 0\n",
    "for shuffle in ['shuf_smp', 'shuf_gp']:\n",
    "    for exp in [0, .3, .5, 1, 1.5]:\n",
    "        for itry in range(ntries):\n",
    "            # take one from each cluster\n",
    "            new100 = [gp.sample(1)]\n",
    "            nrest = 100 - len(new100[0])\n",
    "\n",
    "            # calculate cluster weights\n",
    "            wloss = clusters.loss ** exp\n",
    "            p = wloss / wloss.sum()\n",
    "            nsmp = np.random.multinomial(nrest, p)\n",
    "\n",
    "            # sample from the groups\n",
    "            for (c, g), n in zip(gp, nsmp):\n",
    "                new100.append(g.sample(min(n, len(g))))\n",
    "\n",
    "            new100 = pd.concat(new100)\n",
    "            remaining = 100 - len(new100)\n",
    "            if remaining > 0:\n",
    "                irest = np.random.choice([ii for ii in top_samples.index if ii not in new100.index], remaining, replace=False)\n",
    "                new100 = pd.concat((new100, top_samples.loc[irest]))\n",
    "\n",
    "            if shuffle == 'shuf_smp':\n",
    "                new100 = new100.sample(frac=1)\n",
    "            elif shuffle == 'shuf_gp':\n",
    "                new100 = shuffle_by_group(new100, 'c')\n",
    "            else:\n",
    "                assert False\n",
    "                \n",
    "            isave_root = save_root / f'{iexp:02d}_exp_{exp}_{shuffle}' / f'{itry:02d}'\n",
    "            if not isave_root.exists():\n",
    "                isave_root.mkdir(parents=True)\n",
    "            save_list(new100, isave_root)\n",
    "        iexp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1bbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e91b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56998db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
